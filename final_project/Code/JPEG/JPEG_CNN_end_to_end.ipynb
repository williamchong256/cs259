{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"JPEG_CNN_end_to_end.ipynb","provenance":[{"file_id":"11qcf0IyOwZlnD1l1pyroIO__K0F8lxFo","timestamp":1623481991786},{"file_id":"https://github.com/kunalrdeshmukh/End-to-end-compression/blob/master/imagenet_end_to_end_image_compression.ipynb","timestamp":1623130255406}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"BWvNf8nqIvZ7"},"source":["# References\n","This code is referenced from Kunal Deshmukh's GitHub Repository:\n","https://github.com/kunalrdeshmukh/End-to-end-compression\n","\n","It was a big help as it helped speed-up implementation of CNN End-to-End Compression Framework:\n","https://arxiv.org/abs/1708.00838\n"]},{"cell_type":"markdown","metadata":{"id":"SA29G6TcgrZz"},"source":["Image Compression framework :\n","\n","[End to end image compression framework ](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7999241)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D98I4TsNVy2W","executionInfo":{"status":"ok","timestamp":1623392981050,"user_tz":420,"elapsed":10267,"user":{"displayName":"William Chong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEeniy7KHOq_LWU1clciKEJDeHZlf8TN47vQoL3g=s64","userId":"04480825896771140250"}},"outputId":"f616f0e1-a878-487d-f9ac-be6358fca137"},"source":["# http://pytorch.org/\n","# !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n","!pip install wheel setuptools\n","\n","from os.path import exists\n","# from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n","# platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n","cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n","accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n","print(accelerator)\n","\n","import torch\n","\n","import os\n","import datetime\n","import torch\n","import torchvision\n","from torch import nn , optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","\n","!pip install Pillow==8.2.0\n","!pip install PIL\n","!pip install image\n","!pip install lmdb\n","from PIL import Image"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.36.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.0.0)\n","cu110\n","Requirement already satisfied: Pillow==8.2.0 in /usr/local/lib/python3.7/dist-packages (8.2.0)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement PIL (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for PIL\u001b[0m\n","Requirement already satisfied: image in /usr/local/lib/python3.7/dist-packages (1.5.33)\n","Requirement already satisfied: django in /usr/local/lib/python3.7/dist-packages (from image) (3.2.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from image) (1.15.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from image) (8.2.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from django->image) (2018.9)\n","Requirement already satisfied: asgiref<4,>=3.3.2 in /usr/local/lib/python3.7/dist-packages (from django->image) (3.3.4)\n","Requirement already satisfied: sqlparse>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from django->image) (0.4.1)\n","Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from asgiref<4,>=3.3.2->django->image) (3.7.4.3)\n","Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (0.99)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"L9KZr6xwIH9V"},"source":["import math\n","irange = range\n","\n","\n","def make_grid(tensor, nrow=8, padding=2,\n","              normalize=False, range=None, scale_each=False, pad_value=0):\n","    \"\"\"Make a grid of images.\n","    Args:\n","        tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)\n","            or a list of images all of the same size.\n","        nrow (int, optional): Number of images displayed in each row of the grid.\n","            The Final grid size is (B / nrow, nrow). Default is 8.\n","        padding (int, optional): amount of padding. Default is 2.\n","        normalize (bool, optional): If True, shift the image to the range (0, 1),\n","            by subtracting the minimum and dividing by the maximum pixel value.\n","        range (tuple, optional): tuple (min, max) where min and max are numbers,\n","            then these numbers are used to normalize the image. By default, min and max\n","            are computed from the tensor.\n","        scale_each (bool, optional): If True, scale each image in the batch of\n","            images separately rather than the (min, max) over all images.\n","        pad_value (float, optional): Value for the padded pixels.\n","    Example:\n","        See this notebook `here <https://gist.github.com/anonymous/bf16430f7750c023141c562f3e9f2a91>`_\n","    \"\"\"\n","    if not (torch.is_tensor(tensor) or\n","            (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):\n","        raise TypeError('tensor or list of tensors expected, got {}'.format(type(tensor)))\n","\n","    # if list of tensors, convert to a 4D mini-batch Tensor\n","    if isinstance(tensor, list):\n","        tensor = torch.stack(tensor, dim=0)\n","\n","    if tensor.dim() == 2:  # single image H x W\n","        tensor = tensor.view(1, tensor.size(0), tensor.size(1))\n","    if tensor.dim() == 3:  # single image\n","        if tensor.size(0) == 1:  # if single-channel, convert to 3-channel\n","            tensor = torch.cat((tensor, tensor, tensor), 0)\n","        tensor = tensor.view(1, tensor.size(0), tensor.size(1), tensor.size(2))\n","\n","    if tensor.dim() == 4 and tensor.size(1) == 1:  # single-channel images\n","        tensor = torch.cat((tensor, tensor, tensor), 1)\n","\n","    if normalize is True:\n","        tensor = tensor.clone()  # avoid modifying tensor in-place\n","        if range is not None:\n","            assert isinstance(range, tuple), \\\n","                \"range has to be a tuple (min, max) if specified. min and max are numbers\"\n","\n","        def norm_ip(img, min, max):\n","            img.clamp_(min=min, max=max)\n","            img.add_(-min).div_(max - min + 1e-5)\n","\n","        def norm_range(t, range):\n","            if range is not None:\n","                norm_ip(t, range[0], range[1])\n","            else:\n","                norm_ip(t, float(t.min()), float(t.max()))\n","\n","        if scale_each is True:\n","            for t in tensor:  # loop over mini-batch dimension\n","                norm_range(t, range)\n","        else:\n","            norm_range(tensor, range)\n","\n","    if tensor.size(0) == 1:\n","        return tensor.squeeze()\n","\n","    # make the mini-batch of images into a grid\n","    nmaps = tensor.size(0)\n","    xmaps = min(nrow, nmaps)\n","    ymaps = int(math.ceil(float(nmaps) / xmaps))\n","    height, width = int(tensor.size(2) + padding), int(tensor.size(3) + padding)\n","    grid = tensor.new(3, height * ymaps + padding, width * xmaps + padding).fill_(pad_value)\n","    k = 0\n","    for y in irange(ymaps):\n","        for x in irange(xmaps):\n","            if k >= nmaps:\n","                break\n","            grid.narrow(1, y * height + padding, height - padding)\\\n","                .narrow(2, x * width + padding, width - padding)\\\n","                .copy_(tensor[k])\n","            k = k + 1\n","    return grid\n","\n","\n","def save_image(tensor, filename, nrow=8, padding=2,\n","               normalize=False, range=None, scale_each=False, pad_value=0):\n","    \"\"\"Save a given Tensor into an image file.\n","    Args:\n","        tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,\n","            saves the tensor as a grid of images by calling ``make_grid``.\n","        **kwargs: Other arguments are documented in ``make_grid``.\n","    \"\"\"\n","    from PIL import Image\n","    grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value,\n","                     normalize=normalize, range=range, scale_each=scale_each)\n","    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n","    im = Image.fromarray(ndarr)\n","    im.save(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8ilCKq1yOCjI","executionInfo":{"status":"ok","timestamp":1623392981059,"user_tz":420,"elapsed":29,"user":{"displayName":"William Chong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEeniy7KHOq_LWU1clciKEJDeHZlf8TN47vQoL3g=s64","userId":"04480825896771140250"}},"outputId":"530cdf13-1dd6-45d8-ffcc-0f696475c851"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v9AlJRmbdMmZ"},"source":["# !pip install pillow --upgrade\n","# %reload_ext autoreload\n","# %autoreload\n","\n","# import cv2\n","# from glob import glob\n","\n","# input_path = '/content/gdrive/MyDrive/cs259 project/unused/jpeg_data/train_data/png_data/8_bit/*.png'\n","\n","# cwd = os.getcwd()\n","# input_dir = os.path.join(cwd, input_path)    \n","# ppms = glob(input_dir)   \n","\n","# img_buf = []\n","\n","# # read png images to img_buf\n","# for count, ppm in enumerate(ppms):\n","#   img = Image.Image.load(ppm)\n","#   img_buf.append(img)\n","#   #img_buf.append(cv2.imread(ppm, 1))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4bqxWZT_TVUe"},"source":["# from google.colab.patches import cv2_imshow\n","# cv2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NG0aiMZcvvcg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623392983928,"user_tz":420,"elapsed":2887,"user":{"displayName":"William Chong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEeniy7KHOq_LWU1clciKEJDeHZlf8TN47vQoL3g=s64","userId":"04480825896771140250"}},"outputId":"87fc4977-9c12-4b68-e154-9305488b7f27"},"source":["!pip install pillow --upgrade\n","%reload_ext autoreload\n","%autoreload\n","\n","img_transform = transforms.Compose([\n","    transforms.Resize((256,256)),\n","    transforms.ToTensor()#,\n","    \n","#     transforms.Normalize((0.5, 0.5, 0.5), (0.24703223,  0.24348513 , 0.26158784))\n","])\n","\n","data_path = '/content/gdrive/MyDrive/cs259 project/unused/jpeg_data/'\n","\n","trainset = torchvision.datasets.ImageFolder( root=data_path+'train_data/', transform = img_transform)\n","train_loader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.ImageFolder( root=data_path+'test_data/', transform=img_transform)\n","test_loader = torch.utils.data.DataLoader(testset, batch_size=8,shuffle=False, num_workers=2)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: pillow in /usr/local/lib/python3.7/dist-packages (8.2.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0a_Skj4x2Sg3"},"source":["# img_transform = transforms.Compose([\n","#     transforms.ToTensor(),\n","#     transforms.Normalize((0.5, 0.5, 0.5), (0.24703223,  0.24348513 , 0.26158784))\n","# ])\n","\n","# trainset = datasets.ImageNet(root='/content/gdrive/MyDrive/cs259 project/imagenet', transform=img_transform)\n","\n","# train_loader = torch.utils.data.DataLoader(trainset, batch_size=16,\n","#                                           shuffle=True, num_workers=2)\n","\n","# testset = datasets.LSUN(root='/content/gdrive/MyDrive/cs259 project/imagenet', train=False,transform=img_transform)\n","\n","# test_loader = torch.utils.data.DataLoader(testset, batch_size=16,shuffle=False, num_workers=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ggg832Bqwbq7","executionInfo":{"status":"ok","timestamp":1623392983932,"user_tz":420,"elapsed":24,"user":{"displayName":"William Chong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEeniy7KHOq_LWU1clciKEJDeHZlf8TN47vQoL3g=s64","userId":"04480825896771140250"}},"outputId":"35fa909a-11e0-4d7b-f7ae-8a0f79361830"},"source":["# code block to text image loader\n","print(len(test_loader))\n","print(train_loader)\n","print(len(trainset))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n","<torch.utils.data.dataloader.DataLoader object at 0x7fd00544f650>\n","46\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KwY8W455pGhk"},"source":["### Parameters"]},{"cell_type":"code","metadata":{"id":"G3n2__mWHBQI"},"source":["CHANNELS = 3\n","HEIGHT = 256\n","WIDTH = 256\n","EPOCHS = 200\n","LOG_INTERVAL = 500"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NOVR5jNOU5A7"},"source":["### Define the network"]},{"cell_type":"code","metadata":{"id":"U5UBMyfgwO5z"},"source":["class Interpolate(nn.Module):\n","    def __init__(self, size, mode):\n","        super(Interpolate, self).__init__()\n","        self.interp = nn.functional.interpolate\n","        self.size = size\n","        self.mode = mode\n","        \n","    def forward(self, x):\n","        x = self.interp(x, size=self.size, mode=self.mode, align_corners=False)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"REJrpjnYFeMm"},"source":["class End_to_end(nn.Module):\n","  def __init__(self):\n","    super(End_to_end, self).__init__()\n","    \n","    # Encoder\n","    # TODO : try with padding = 0\n","    \n","    self.conv1 = nn.Conv2d(CHANNELS, 64, kernel_size=3, stride=1, padding=1)\n","    self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=0)\n","    self.bn1 = nn.BatchNorm2d(64, affine=False)\n","    self.conv3 = nn.Conv2d(64, CHANNELS, kernel_size=3, stride=1, padding=1)\n","    \n","    # Decoder\n","    #TODO : try ConvTranspose2d\n","    self.interpolate = Interpolate(size=HEIGHT, mode='bilinear')\n","    self.deconv1 = nn.Conv2d(CHANNELS, 64, 3, stride=1, padding=1)\n","    self.deconv2 = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n","    self.bn2 = nn.BatchNorm2d(64, affine=False)\n","    \n","    self.deconv_n = nn.Conv2d(64, 64, 3, stride=1, padding=1)\n","    self.bn_n = nn.BatchNorm2d(64, affine=False)\n","\n","    \n","    self.deconv3 = nn.ConvTranspose2d(64, CHANNELS, 3, stride=1, padding=1)\n","    \n","    self.relu = nn.ReLU()\n","  \n","  def encode(self, x):\n","    out = self.relu(self.conv1(x))\n","    out = self.relu(self.conv2(out))\n","    out = self.bn1(out)\n","    return self.conv3(out)\n","    \n","  \n","  def reparameterize(self, mu, logvar):\n","    pass\n","  \n","  def decode(self, z):\n","    upscaled_image = self.interpolate(z)\n","    out = self.relu(self.deconv1(upscaled_image))\n","    out = self.relu(self.deconv2(out))\n","    out = self.bn2(out)\n","    for _ in range(10):\n","      out = self.relu(self.deconv_n(out))\n","      out = self.bn_n(out)\n","    out = self.deconv3(out)\n","    final = upscaled_image + out\n","    return final,out,upscaled_image\n","\n","    \n","  def forward(self, x):\n","    com_img = self.encode(x)\n","    final,out,upscaled_image = self.decode(com_img)\n","    return final, out, upscaled_image, com_img, x\n","\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XG45FHw7MFzW"},"source":["### Optimizer and loss function "]},{"cell_type":"code","metadata":{"id":"RrnHrzxRIP0q"},"source":["CUDA = torch.cuda.is_available()\n","if CUDA:\n","  model = End_to_end().cuda()\n","else :\n","  model = end_to_end()\n","  \n","optimizer = optim.Adam(model.parameters(), lr=1e-3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aj8o41BYW9Lp"},"source":["from skimage.metrics import normalized_root_mse, mean_squared_error\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VfZ9DoJogyOA"},"source":["def loss_function(final_img,residual_img,upscaled_img,com_img,orig_img):\n","  # com_loss = np.sqrt(mean_squared_error(orig_img, final_img))\n","  # rec_loss = np.sqrt(mean_squared_error(residual_img, orig_img - upscaled_img))\n","  com_loss = nn.MSELoss(size_average=False)(orig_img, final_img)\n","  rec_loss = nn.MSELoss(size_average=False)(residual_img,orig_img-upscaled_img)\n","  \n","  return com_loss + rec_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u9QRvcpIMPGU"},"source":["### Train , Test functions"]},{"cell_type":"code","metadata":{"id":"dWsyut4Kbruf"},"source":["def train(epoch):\n","    model.train()\n","    train_loss = 0\n","    for batch_idx, (data, _) in enumerate(train_loader):\n","        data = Variable(data)\n","        optimizer.zero_grad()\n","        final, residual_img, upscaled_image, com_img, orig_im = model(data.cuda())\n","        loss = loss_function(final, residual_img, upscaled_image, com_img, orig_im)\n","        loss.backward()\n","        train_loss += loss.data\n","        optimizer.step()\n","        if batch_idx % LOG_INTERVAL == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n","                epoch, batch_idx * len(data), len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader),\n","                loss.data / len(data)))\n","            print(datetime.datetime.now().time())\n","\n","    print('====> Epoch: {} Average loss: {:.4f}'.format(\n","          epoch, train_loss / len(train_loader.dataset)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mpg44nfscmcG"},"source":["def test(epoch):\n","  \n","  model.eval()\n","  test_loss = 0\n","  for i, (data, _) in enumerate(test_loader):\n","        data = Variable(data, volatile=True)\n","        final, residual_img, upscaled_image, com_img, orig_im = model(data.cuda())\n","        test_loss += loss_function(final, residual_img, upscaled_image, com_img, orig_im).data\n","        if epoch == EPOCHS and i == 0:\n","#             save_image(final.data[0],'reconstruction_final',nrow=8)\n","#             save_image(com_img.data[0],'com_img',nrow=8)\n","            n = min(data.size(0), 6)\n","            print(\"saving the image \"+str(n))\n","            comparison = torch.cat([data[:n],\n","              final[:n].cpu()])\n","            comparison = comparison.cpu()\n","#             print(comparison.data)\n","            save_image(com_img[:n].data,\n","                       '/content/gdrive/MyDrive/cs259 project/results2/compressed_' + str(epoch) +'.png', nrow=n)\n","            save_image(comparison.data,\n","                       '/content/gdrive/MyDrive/cs259 project/results2/reconstruction_' + str(epoch) +'.png', nrow=n)\n","\n","  test_loss /= len(test_loader.dataset)\n","  print('====> Test set loss: {:.4f}'.format(test_loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sgm066wKMWVU"},"source":["### Run the code"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vwuk-i3jcnIQ","executionInfo":{"status":"ok","timestamp":1623394876922,"user_tz":420,"elapsed":650676,"user":{"displayName":"William Chong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEeniy7KHOq_LWU1clciKEJDeHZlf8TN47vQoL3g=s64","userId":"04480825896771140250"}},"outputId":"4815b53c-947c-4777-904d-5a2b10f3c9eb"},"source":["for epoch in range(1, EPOCHS+1):\n","    train(epoch)\n","    test(epoch)\n","#         sample = Variable(torch.randn(64, args.hidden_size),16)\n","#         sample = model.decode(sample)\n","#         print(\"saving im\")\n","#         save_image(sample.data.view(64, 3, 32, 32),\n","# 'sample_' + str(epoch) + '.png')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 1 [0/46 (0%)]\tLoss: 3214035.750000\n","06:29:49.372916\n","====> Epoch: 1 Average loss: 1757131.1250\n","====> Test set loss: 1902227.3750\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch: 2 [0/46 (0%)]\tLoss: 476586.625000\n","06:29:58.936052\n","====> Epoch: 2 Average loss: 325194.9688\n","====> Test set loss: 158905.4375\n","Train Epoch: 3 [0/46 (0%)]\tLoss: 187501.281250\n","06:30:07.841580\n","====> Epoch: 3 Average loss: 167400.0781\n","====> Test set loss: 66268.9375\n","Train Epoch: 4 [0/46 (0%)]\tLoss: 159165.781250\n","06:30:17.860395\n","====> Epoch: 4 Average loss: 130539.5234\n","====> Test set loss: 72783.9141\n","Train Epoch: 5 [0/46 (0%)]\tLoss: 126496.812500\n","06:30:27.253402\n","====> Epoch: 5 Average loss: 106828.5703\n","====> Test set loss: 55331.2266\n","Train Epoch: 6 [0/46 (0%)]\tLoss: 87195.953125\n","06:30:37.231818\n","====> Epoch: 6 Average loss: 88319.8516\n","====> Test set loss: 73177.5469\n","Train Epoch: 7 [0/46 (0%)]\tLoss: 67256.250000\n","06:30:46.016929\n","====> Epoch: 7 Average loss: 78211.3516\n","====> Test set loss: 112566.8594\n","Train Epoch: 8 [0/46 (0%)]\tLoss: 86585.796875\n","06:30:55.001681\n","====> Epoch: 8 Average loss: 70144.7734\n","====> Test set loss: 70603.5156\n","Train Epoch: 9 [0/46 (0%)]\tLoss: 72087.062500\n","06:31:05.601570\n","====> Epoch: 9 Average loss: 60623.8125\n","====> Test set loss: 73293.9609\n","Train Epoch: 10 [0/46 (0%)]\tLoss: 36607.906250\n","06:31:15.208922\n","====> Epoch: 10 Average loss: 56171.8438\n","====> Test set loss: 74452.4375\n","Train Epoch: 11 [0/46 (0%)]\tLoss: 34772.953125\n","06:31:25.323703\n","====> Epoch: 11 Average loss: 53857.3008\n","====> Test set loss: 98265.7578\n","Train Epoch: 12 [0/46 (0%)]\tLoss: 60792.195312\n","06:31:33.640144\n","====> Epoch: 12 Average loss: 45983.1211\n","====> Test set loss: 71642.3906\n","Train Epoch: 13 [0/46 (0%)]\tLoss: 47293.148438\n","06:31:43.632219\n","====> Epoch: 13 Average loss: 42795.9336\n","====> Test set loss: 60318.3438\n","Train Epoch: 14 [0/46 (0%)]\tLoss: 45619.601562\n","06:31:52.916606\n","====> Epoch: 14 Average loss: 41411.1328\n","====> Test set loss: 48092.5117\n","Train Epoch: 15 [0/46 (0%)]\tLoss: 34658.250000\n","06:32:02.481815\n","====> Epoch: 15 Average loss: 38252.9922\n","====> Test set loss: 59321.1094\n","Train Epoch: 16 [0/46 (0%)]\tLoss: 25472.705078\n","06:32:12.124985\n","====> Epoch: 16 Average loss: 35907.6211\n","====> Test set loss: 42422.7500\n","Train Epoch: 17 [0/46 (0%)]\tLoss: 26526.050781\n","06:32:21.243386\n","====> Epoch: 17 Average loss: 33888.0391\n","====> Test set loss: 42097.4336\n","Train Epoch: 18 [0/46 (0%)]\tLoss: 27329.753906\n","06:32:30.370871\n","====> Epoch: 18 Average loss: 33480.1523\n","====> Test set loss: 49133.1523\n","Train Epoch: 19 [0/46 (0%)]\tLoss: 24291.992188\n","06:32:39.933885\n","====> Epoch: 19 Average loss: 28298.1777\n","====> Test set loss: 33708.8281\n","Train Epoch: 20 [0/46 (0%)]\tLoss: 36052.738281\n","06:32:49.773317\n","====> Epoch: 20 Average loss: 26952.8457\n","====> Test set loss: 42417.7695\n","Train Epoch: 21 [0/46 (0%)]\tLoss: 27286.171875\n","06:32:58.628709\n","====> Epoch: 21 Average loss: 25713.3672\n","====> Test set loss: 30488.3340\n","Train Epoch: 22 [0/46 (0%)]\tLoss: 19482.156250\n","06:33:08.218756\n","====> Epoch: 22 Average loss: 23662.3398\n","====> Test set loss: 32127.6484\n","Train Epoch: 23 [0/46 (0%)]\tLoss: 29425.857422\n","06:33:16.378926\n","====> Epoch: 23 Average loss: 22374.5410\n","====> Test set loss: 30318.2305\n","Train Epoch: 24 [0/46 (0%)]\tLoss: 23970.636719\n","06:33:25.768568\n","====> Epoch: 24 Average loss: 20662.3730\n","====> Test set loss: 24361.8066\n","Train Epoch: 25 [0/46 (0%)]\tLoss: 23969.902344\n","06:33:35.753755\n","====> Epoch: 25 Average loss: 20496.7422\n","====> Test set loss: 29872.5723\n","Train Epoch: 26 [0/46 (0%)]\tLoss: 27280.726562\n","06:33:45.102682\n","====> Epoch: 26 Average loss: 19989.9629\n","====> Test set loss: 20694.4375\n","Train Epoch: 27 [0/46 (0%)]\tLoss: 15808.625000\n","06:33:54.356579\n","====> Epoch: 27 Average loss: 18049.3418\n","====> Test set loss: 20929.3574\n","Train Epoch: 28 [0/46 (0%)]\tLoss: 12763.191406\n","06:34:04.277359\n","====> Epoch: 28 Average loss: 17311.2441\n","====> Test set loss: 20862.2109\n","Train Epoch: 29 [0/46 (0%)]\tLoss: 14575.962891\n","06:34:13.500278\n","====> Epoch: 29 Average loss: 15477.1475\n","====> Test set loss: 18752.9570\n","Train Epoch: 30 [0/46 (0%)]\tLoss: 16329.435547\n","06:34:22.420520\n","====> Epoch: 30 Average loss: 14596.3506\n","====> Test set loss: 17106.9590\n","Train Epoch: 31 [0/46 (0%)]\tLoss: 13705.099609\n","06:34:31.653151\n","====> Epoch: 31 Average loss: 13983.8438\n","====> Test set loss: 18915.3574\n","Train Epoch: 32 [0/46 (0%)]\tLoss: 21794.380859\n","06:34:40.549690\n","====> Epoch: 32 Average loss: 13652.0713\n","====> Test set loss: 18225.4746\n","Train Epoch: 33 [0/46 (0%)]\tLoss: 11204.726562\n","06:34:50.989596\n","====> Epoch: 33 Average loss: 13171.1660\n","====> Test set loss: 15255.3398\n","Train Epoch: 34 [0/46 (0%)]\tLoss: 11742.223633\n","06:35:00.444303\n","====> Epoch: 34 Average loss: 12540.9170\n","====> Test set loss: 15232.3262\n","Train Epoch: 35 [0/46 (0%)]\tLoss: 10359.116211\n","06:35:08.911293\n","====> Epoch: 35 Average loss: 12493.3398\n","====> Test set loss: 14639.5361\n","Train Epoch: 36 [0/46 (0%)]\tLoss: 13701.474609\n","06:35:18.609479\n","====> Epoch: 36 Average loss: 12472.6631\n","====> Test set loss: 11448.2598\n","Train Epoch: 37 [0/46 (0%)]\tLoss: 14859.307617\n","06:35:27.021138\n","====> Epoch: 37 Average loss: 13704.5977\n","====> Test set loss: 18086.8027\n","Train Epoch: 38 [0/46 (0%)]\tLoss: 10895.261719\n","06:35:36.240098\n","====> Epoch: 38 Average loss: 10905.9961\n","====> Test set loss: 9222.8301\n","Train Epoch: 39 [0/46 (0%)]\tLoss: 10634.921875\n","06:35:46.138768\n","====> Epoch: 39 Average loss: 10853.8281\n","====> Test set loss: 12216.3867\n","Train Epoch: 40 [0/46 (0%)]\tLoss: 7303.397461\n","06:35:55.245068\n","====> Epoch: 40 Average loss: 9654.2539\n","====> Test set loss: 8277.9170\n","Train Epoch: 41 [0/46 (0%)]\tLoss: 7797.994141\n","06:36:04.401467\n","====> Epoch: 41 Average loss: 10234.4600\n","====> Test set loss: 9061.6152\n","Train Epoch: 42 [0/46 (0%)]\tLoss: 10021.122070\n","06:36:13.563992\n","====> Epoch: 42 Average loss: 8881.5205\n","====> Test set loss: 9790.3457\n","Train Epoch: 43 [0/46 (0%)]\tLoss: 10755.123047\n","06:36:23.491362\n","====> Epoch: 43 Average loss: 11951.6514\n","====> Test set loss: 15509.7793\n","Train Epoch: 44 [0/46 (0%)]\tLoss: 8949.929688\n","06:36:32.381973\n","====> Epoch: 44 Average loss: 8730.0928\n","====> Test set loss: 10538.0674\n","Train Epoch: 45 [0/46 (0%)]\tLoss: 6668.413574\n","06:36:43.009800\n","====> Epoch: 45 Average loss: 7546.9194\n","====> Test set loss: 9129.9521\n","Train Epoch: 46 [0/46 (0%)]\tLoss: 12052.197266\n","06:36:51.676912\n","====> Epoch: 46 Average loss: 6749.8101\n","====> Test set loss: 8221.4209\n","Train Epoch: 47 [0/46 (0%)]\tLoss: 9754.122070\n","06:37:00.595365\n","====> Epoch: 47 Average loss: 7929.3208\n","====> Test set loss: 6692.6699\n","Train Epoch: 48 [0/46 (0%)]\tLoss: 4455.800781\n","06:37:10.657505\n","====> Epoch: 48 Average loss: 8176.1841\n","====> Test set loss: 7989.7896\n","Train Epoch: 49 [0/46 (0%)]\tLoss: 3721.471680\n","06:37:20.074954\n","====> Epoch: 49 Average loss: 6262.8647\n","====> Test set loss: 6780.4082\n","Train Epoch: 50 [0/46 (0%)]\tLoss: 10336.484375\n","06:37:29.110324\n","====> Epoch: 50 Average loss: 7075.2163\n","====> Test set loss: 7914.8965\n","Train Epoch: 51 [0/46 (0%)]\tLoss: 5593.839844\n","06:37:38.425609\n","====> Epoch: 51 Average loss: 6041.9199\n","====> Test set loss: 5384.6753\n","Train Epoch: 52 [0/46 (0%)]\tLoss: 4826.622070\n","06:37:49.059897\n","====> Epoch: 52 Average loss: 5809.5938\n","====> Test set loss: 6989.3052\n","Train Epoch: 53 [0/46 (0%)]\tLoss: 7068.907227\n","06:37:58.816747\n","====> Epoch: 53 Average loss: 6559.2710\n","====> Test set loss: 7498.0283\n","Train Epoch: 54 [0/46 (0%)]\tLoss: 6357.429688\n","06:38:07.695894\n","====> Epoch: 54 Average loss: 5597.9028\n","====> Test set loss: 8097.0942\n","Train Epoch: 55 [0/46 (0%)]\tLoss: 5032.504883\n","06:38:16.880685\n","====> Epoch: 55 Average loss: 5201.3179\n","====> Test set loss: 6202.2100\n","Train Epoch: 56 [0/46 (0%)]\tLoss: 5007.797363\n","06:38:27.326142\n","====> Epoch: 56 Average loss: 5770.8091\n","====> Test set loss: 5983.0283\n","Train Epoch: 57 [0/46 (0%)]\tLoss: 13464.540039\n","06:38:35.338070\n","====> Epoch: 57 Average loss: 5651.4189\n","====> Test set loss: 6521.7847\n","Train Epoch: 58 [0/46 (0%)]\tLoss: 3968.791992\n","06:38:45.178587\n","====> Epoch: 58 Average loss: 5052.5054\n","====> Test set loss: 5143.8667\n","Train Epoch: 59 [0/46 (0%)]\tLoss: 7130.664062\n","06:38:53.640292\n","====> Epoch: 59 Average loss: 5829.4106\n","====> Test set loss: 7202.4087\n","Train Epoch: 60 [0/46 (0%)]\tLoss: 10584.670898\n","06:39:04.162446\n","====> Epoch: 60 Average loss: 6197.9912\n","====> Test set loss: 6377.4658\n","Train Epoch: 61 [0/46 (0%)]\tLoss: 5792.134766\n","06:39:12.527529\n","====> Epoch: 61 Average loss: 5476.6484\n","====> Test set loss: 5562.7056\n","Train Epoch: 62 [0/46 (0%)]\tLoss: 6478.877441\n","06:39:22.048930\n","====> Epoch: 62 Average loss: 5574.6484\n","====> Test set loss: 4961.9912\n","Train Epoch: 63 [0/46 (0%)]\tLoss: 3734.219727\n","06:39:32.169919\n","====> Epoch: 63 Average loss: 5427.9082\n","====> Test set loss: 5811.8472\n","Train Epoch: 64 [0/46 (0%)]\tLoss: 3285.945312\n","06:39:41.501677\n","====> Epoch: 64 Average loss: 5004.3662\n","====> Test set loss: 4700.6899\n","Train Epoch: 65 [0/46 (0%)]\tLoss: 8072.055176\n","06:39:50.850564\n","====> Epoch: 65 Average loss: 6056.7324\n","====> Test set loss: 4797.2285\n","Train Epoch: 66 [0/46 (0%)]\tLoss: 5968.762695\n","06:39:59.975981\n","====> Epoch: 66 Average loss: 5249.7109\n","====> Test set loss: 6645.2446\n","Train Epoch: 67 [0/46 (0%)]\tLoss: 4531.368164\n","06:40:09.331909\n","====> Epoch: 67 Average loss: 4868.8359\n","====> Test set loss: 4712.3252\n","Train Epoch: 68 [0/46 (0%)]\tLoss: 4680.972656\n","06:40:18.705135\n","====> Epoch: 68 Average loss: 4366.9116\n","====> Test set loss: 4499.8750\n","Train Epoch: 69 [0/46 (0%)]\tLoss: 5187.360352\n","06:40:28.584634\n","====> Epoch: 69 Average loss: 4720.6768\n","====> Test set loss: 4009.5288\n","Train Epoch: 70 [0/46 (0%)]\tLoss: 6806.610352\n","06:40:37.640800\n","====> Epoch: 70 Average loss: 5472.1426\n","====> Test set loss: 6700.7983\n","Train Epoch: 71 [0/46 (0%)]\tLoss: 5791.793945\n","06:40:47.192892\n","====> Epoch: 71 Average loss: 4513.6294\n","====> Test set loss: 4366.1436\n","Train Epoch: 72 [0/46 (0%)]\tLoss: 4493.976562\n","06:40:56.730487\n","====> Epoch: 72 Average loss: 4378.9248\n","====> Test set loss: 4005.4722\n","Train Epoch: 73 [0/46 (0%)]\tLoss: 4190.978516\n","06:41:06.260291\n","====> Epoch: 73 Average loss: 4653.4497\n","====> Test set loss: 4348.1812\n","Train Epoch: 74 [0/46 (0%)]\tLoss: 3897.763672\n","06:41:15.176740\n","====> Epoch: 74 Average loss: 4523.2788\n","====> Test set loss: 4583.5908\n","Train Epoch: 75 [0/46 (0%)]\tLoss: 4511.404297\n","06:41:24.162268\n","====> Epoch: 75 Average loss: 4709.0659\n","====> Test set loss: 3777.5225\n","Train Epoch: 76 [0/46 (0%)]\tLoss: 6433.692871\n","06:41:34.788543\n","====> Epoch: 76 Average loss: 4934.2954\n","====> Test set loss: 4365.7744\n","Train Epoch: 77 [0/46 (0%)]\tLoss: 4923.982422\n","06:41:43.162582\n","====> Epoch: 77 Average loss: 4473.9009\n","====> Test set loss: 4234.3584\n","Train Epoch: 78 [0/46 (0%)]\tLoss: 5315.008789\n","06:41:52.739472\n","====> Epoch: 78 Average loss: 4526.5591\n","====> Test set loss: 3951.8887\n","Train Epoch: 79 [0/46 (0%)]\tLoss: 6082.106445\n","06:42:02.388611\n","====> Epoch: 79 Average loss: 5917.5610\n","====> Test set loss: 5879.3701\n","Train Epoch: 80 [0/46 (0%)]\tLoss: 6223.998535\n","06:42:11.272588\n","====> Epoch: 80 Average loss: 5395.0591\n","====> Test set loss: 5285.1240\n","Train Epoch: 81 [0/46 (0%)]\tLoss: 5769.791016\n","06:42:21.191826\n","====> Epoch: 81 Average loss: 4840.0781\n","====> Test set loss: 4597.1968\n","Train Epoch: 82 [0/46 (0%)]\tLoss: 4276.696289\n","06:42:30.679601\n","====> Epoch: 82 Average loss: 5195.0122\n","====> Test set loss: 6338.9546\n","Train Epoch: 83 [0/46 (0%)]\tLoss: 5191.920898\n","06:42:39.777155\n","====> Epoch: 83 Average loss: 4823.8853\n","====> Test set loss: 3922.9302\n","Train Epoch: 84 [0/46 (0%)]\tLoss: 5062.684570\n","06:42:49.032875\n","====> Epoch: 84 Average loss: 3973.1553\n","====> Test set loss: 3685.0852\n","Train Epoch: 85 [0/46 (0%)]\tLoss: 9563.550781\n","06:42:58.457822\n","====> Epoch: 85 Average loss: 5104.6821\n","====> Test set loss: 4850.7007\n","Train Epoch: 86 [0/46 (0%)]\tLoss: 7461.236816\n","06:43:08.345832\n","====> Epoch: 86 Average loss: 4565.1733\n","====> Test set loss: 3930.3950\n","Train Epoch: 87 [0/46 (0%)]\tLoss: 5057.158691\n","06:43:16.848104\n","====> Epoch: 87 Average loss: 4750.4565\n","====> Test set loss: 10848.8711\n","Train Epoch: 88 [0/46 (0%)]\tLoss: 5664.105469\n","06:43:27.647726\n","====> Epoch: 88 Average loss: 5091.9463\n","====> Test set loss: 6039.0913\n","Train Epoch: 89 [0/46 (0%)]\tLoss: 3144.038086\n","06:43:36.066426\n","====> Epoch: 89 Average loss: 3910.0334\n","====> Test set loss: 3357.5151\n","Train Epoch: 90 [0/46 (0%)]\tLoss: 3334.128906\n","06:43:45.649441\n","====> Epoch: 90 Average loss: 3559.5161\n","====> Test set loss: 2958.5520\n","Train Epoch: 91 [0/46 (0%)]\tLoss: 2936.176758\n","06:43:54.776996\n","====> Epoch: 91 Average loss: 4015.3364\n","====> Test set loss: 24412.5449\n","Train Epoch: 92 [0/46 (0%)]\tLoss: 5175.183594\n","06:44:04.032935\n","====> Epoch: 92 Average loss: 7039.2656\n","====> Test set loss: 8769.7217\n","Train Epoch: 93 [0/46 (0%)]\tLoss: 6840.064453\n","06:44:13.339753\n","====> Epoch: 93 Average loss: 6643.1631\n","====> Test set loss: 9742.5986\n","Train Epoch: 94 [0/46 (0%)]\tLoss: 6228.906250\n","06:44:22.362806\n","====> Epoch: 94 Average loss: 6871.7666\n","====> Test set loss: 5340.5029\n","Train Epoch: 95 [0/46 (0%)]\tLoss: 4795.049316\n","06:44:31.764710\n","====> Epoch: 95 Average loss: 7489.2646\n","====> Test set loss: 5604.0439\n","Train Epoch: 96 [0/46 (0%)]\tLoss: 5077.835938\n","06:44:40.230914\n","====> Epoch: 96 Average loss: 5543.6943\n","====> Test set loss: 3672.7317\n","Train Epoch: 97 [0/46 (0%)]\tLoss: 3930.162598\n","06:44:50.534506\n","====> Epoch: 97 Average loss: 6059.2100\n","====> Test set loss: 4266.0420\n","Train Epoch: 98 [0/46 (0%)]\tLoss: 4393.054688\n","06:44:59.973286\n","====> Epoch: 98 Average loss: 4275.7285\n","====> Test set loss: 2871.2842\n","Train Epoch: 99 [0/46 (0%)]\tLoss: 2501.061035\n","06:45:09.019607\n","====> Epoch: 99 Average loss: 4064.1631\n","====> Test set loss: 3319.2815\n","Train Epoch: 100 [0/46 (0%)]\tLoss: 4056.798828\n","06:45:17.844679\n","====> Epoch: 100 Average loss: 3749.4011\n","====> Test set loss: 3502.1519\n","Train Epoch: 101 [0/46 (0%)]\tLoss: 3794.620117\n","06:45:27.359009\n","====> Epoch: 101 Average loss: 3636.3149\n","====> Test set loss: 3990.6865\n","Train Epoch: 102 [0/46 (0%)]\tLoss: 8014.526367\n","06:45:37.482834\n","====> Epoch: 102 Average loss: 4857.4355\n","====> Test set loss: 3290.1438\n","Train Epoch: 103 [0/46 (0%)]\tLoss: 8276.320312\n","06:45:45.567466\n","====> Epoch: 103 Average loss: 5830.7930\n","====> Test set loss: 4599.8857\n","Train Epoch: 104 [0/46 (0%)]\tLoss: 4464.088867\n","06:45:54.968655\n","====> Epoch: 104 Average loss: 3994.4124\n","====> Test set loss: 3211.6926\n","Train Epoch: 105 [0/46 (0%)]\tLoss: 4693.267578\n","06:46:05.078427\n","====> Epoch: 105 Average loss: 3383.3252\n","====> Test set loss: 3075.7747\n","Train Epoch: 106 [0/46 (0%)]\tLoss: 3204.263672\n","06:46:13.866452\n","====> Epoch: 106 Average loss: 3892.3711\n","====> Test set loss: 3178.9353\n","Train Epoch: 107 [0/46 (0%)]\tLoss: 4209.190430\n","06:46:23.494695\n","====> Epoch: 107 Average loss: 3459.9365\n","====> Test set loss: 2681.2422\n","Train Epoch: 108 [0/46 (0%)]\tLoss: 2711.971436\n","06:46:32.979628\n","====> Epoch: 108 Average loss: 3555.2385\n","====> Test set loss: 2658.9756\n","Train Epoch: 109 [0/46 (0%)]\tLoss: 3672.044922\n","06:46:42.181969\n","====> Epoch: 109 Average loss: 3829.3193\n","====> Test set loss: 2975.3494\n","Train Epoch: 110 [0/46 (0%)]\tLoss: 3029.180664\n","06:46:51.720934\n","====> Epoch: 110 Average loss: 3724.7437\n","====> Test set loss: 2622.2915\n","Train Epoch: 111 [0/46 (0%)]\tLoss: 3514.274658\n","06:47:01.389263\n","====> Epoch: 111 Average loss: 3986.5229\n","====> Test set loss: 2320.1162\n","Train Epoch: 112 [0/46 (0%)]\tLoss: 3107.977539\n","06:47:10.433851\n","====> Epoch: 112 Average loss: 3514.2671\n","====> Test set loss: 3324.3345\n","Train Epoch: 113 [0/46 (0%)]\tLoss: 4860.449219\n","06:47:19.647037\n","====> Epoch: 113 Average loss: 3460.9851\n","====> Test set loss: 3024.8596\n","Train Epoch: 114 [0/46 (0%)]\tLoss: 3444.609375\n","06:47:29.455391\n","====> Epoch: 114 Average loss: 3185.1562\n","====> Test set loss: 3903.4817\n","Train Epoch: 115 [0/46 (0%)]\tLoss: 4063.835449\n","06:47:38.649655\n","====> Epoch: 115 Average loss: 3942.6440\n","====> Test set loss: 4811.8657\n","Train Epoch: 116 [0/46 (0%)]\tLoss: 2901.422852\n","06:47:47.737807\n","====> Epoch: 116 Average loss: 3539.2202\n","====> Test set loss: 2775.9275\n","Train Epoch: 117 [0/46 (0%)]\tLoss: 3119.109619\n","06:47:56.764650\n","====> Epoch: 117 Average loss: 3548.5596\n","====> Test set loss: 2559.6436\n","Train Epoch: 118 [0/46 (0%)]\tLoss: 2924.246094\n","06:48:07.186524\n","====> Epoch: 118 Average loss: 3352.3420\n","====> Test set loss: 2684.0642\n","Train Epoch: 119 [0/46 (0%)]\tLoss: 1901.833008\n","06:48:15.399967\n","====> Epoch: 119 Average loss: 3228.0808\n","====> Test set loss: 3147.5776\n","Train Epoch: 120 [0/46 (0%)]\tLoss: 3438.367188\n","06:48:24.538600\n","====> Epoch: 120 Average loss: 4117.8247\n","====> Test set loss: 4092.8074\n","Train Epoch: 121 [0/46 (0%)]\tLoss: 3360.843262\n","06:48:33.560893\n","====> Epoch: 121 Average loss: 3888.2073\n","====> Test set loss: 3318.7205\n","Train Epoch: 122 [0/46 (0%)]\tLoss: 2611.985840\n","06:48:43.753505\n","====> Epoch: 122 Average loss: 4158.8311\n","====> Test set loss: 4182.6772\n","Train Epoch: 123 [0/46 (0%)]\tLoss: 2470.622803\n","06:48:53.098208\n","====> Epoch: 123 Average loss: 3158.8716\n","====> Test set loss: 3320.7144\n","Train Epoch: 124 [0/46 (0%)]\tLoss: 3290.991211\n","06:49:03.200628\n","====> Epoch: 124 Average loss: 3404.5310\n","====> Test set loss: 2603.7300\n","Train Epoch: 125 [0/46 (0%)]\tLoss: 3655.376953\n","06:49:12.441807\n","====> Epoch: 125 Average loss: 4190.0645\n","====> Test set loss: 5772.0239\n","Train Epoch: 126 [0/46 (0%)]\tLoss: 3977.591309\n","06:49:21.736791\n","====> Epoch: 126 Average loss: 4259.4106\n","====> Test set loss: 3416.2131\n","Train Epoch: 127 [0/46 (0%)]\tLoss: 5395.299805\n","06:49:31.205617\n","====> Epoch: 127 Average loss: 4680.5483\n","====> Test set loss: 3006.7932\n","Train Epoch: 128 [0/46 (0%)]\tLoss: 2338.608887\n","06:49:40.208358\n","====> Epoch: 128 Average loss: 3207.9453\n","====> Test set loss: 3222.8782\n","Train Epoch: 129 [0/46 (0%)]\tLoss: 2856.414062\n","06:49:50.492044\n","====> Epoch: 129 Average loss: 4085.8342\n","====> Test set loss: 2689.5046\n","Train Epoch: 130 [0/46 (0%)]\tLoss: 2521.775391\n","06:49:58.938056\n","====> Epoch: 130 Average loss: 3665.3306\n","====> Test set loss: 2732.8718\n","Train Epoch: 131 [0/46 (0%)]\tLoss: 3695.111816\n","06:50:09.210019\n","====> Epoch: 131 Average loss: 4008.7170\n","====> Test set loss: 2234.0173\n","Train Epoch: 132 [0/46 (0%)]\tLoss: 4255.235840\n","06:50:18.624159\n","====> Epoch: 132 Average loss: 4257.8301\n","====> Test set loss: 3009.3794\n","Train Epoch: 133 [0/46 (0%)]\tLoss: 3996.810303\n","06:50:27.572944\n","====> Epoch: 133 Average loss: 4367.1499\n","====> Test set loss: 4721.4512\n","Train Epoch: 134 [0/46 (0%)]\tLoss: 4094.939209\n","06:50:37.362951\n","====> Epoch: 134 Average loss: 3688.1997\n","====> Test set loss: 3665.8271\n","Train Epoch: 135 [0/46 (0%)]\tLoss: 3381.188477\n","06:50:46.749585\n","====> Epoch: 135 Average loss: 4197.2710\n","====> Test set loss: 4444.9902\n","Train Epoch: 136 [0/46 (0%)]\tLoss: 3013.681641\n","06:50:56.757801\n","====> Epoch: 136 Average loss: 3578.1133\n","====> Test set loss: 2747.7612\n","Train Epoch: 137 [0/46 (0%)]\tLoss: 2896.540527\n","06:51:05.928271\n","====> Epoch: 137 Average loss: 3450.9565\n","====> Test set loss: 2850.4924\n","Train Epoch: 138 [0/46 (0%)]\tLoss: 6372.125000\n","06:51:15.348304\n","====> Epoch: 138 Average loss: 4007.7422\n","====> Test set loss: 3059.8909\n","Train Epoch: 139 [0/46 (0%)]\tLoss: 3272.369141\n","06:51:24.750773\n","====> Epoch: 139 Average loss: 4048.3259\n","====> Test set loss: 3226.1685\n","Train Epoch: 140 [0/46 (0%)]\tLoss: 5501.015137\n","06:51:34.758266\n","====> Epoch: 140 Average loss: 3731.8479\n","====> Test set loss: 2521.7598\n","Train Epoch: 141 [0/46 (0%)]\tLoss: 5084.943359\n","06:51:44.011147\n","====> Epoch: 141 Average loss: 4158.2476\n","====> Test set loss: 3638.9946\n","Train Epoch: 142 [0/46 (0%)]\tLoss: 3261.454590\n","06:51:54.647673\n","====> Epoch: 142 Average loss: 3130.8838\n","====> Test set loss: 2422.4761\n","Train Epoch: 143 [0/46 (0%)]\tLoss: 4811.422852\n","06:52:04.116434\n","====> Epoch: 143 Average loss: 3291.9573\n","====> Test set loss: 2628.9954\n","Train Epoch: 144 [0/46 (0%)]\tLoss: 2502.284912\n","06:52:13.839986\n","====> Epoch: 144 Average loss: 2641.7866\n","====> Test set loss: 2286.7927\n","Train Epoch: 145 [0/46 (0%)]\tLoss: 3324.256348\n","06:52:23.687920\n","====> Epoch: 145 Average loss: 2664.7561\n","====> Test set loss: 2564.2214\n","Train Epoch: 146 [0/46 (0%)]\tLoss: 3623.544922\n","06:52:33.175477\n","====> Epoch: 146 Average loss: 3746.8923\n","====> Test set loss: 3702.6465\n","Train Epoch: 147 [0/46 (0%)]\tLoss: 5122.761719\n","06:52:42.911129\n","====> Epoch: 147 Average loss: 4713.5820\n","====> Test set loss: 3427.9285\n","Train Epoch: 148 [0/46 (0%)]\tLoss: 7975.683594\n","06:52:52.984376\n","====> Epoch: 148 Average loss: 4471.7666\n","====> Test set loss: 2980.1396\n","Train Epoch: 149 [0/46 (0%)]\tLoss: 5686.654297\n","06:53:02.840172\n","====> Epoch: 149 Average loss: 3529.7415\n","====> Test set loss: 7493.4395\n","Train Epoch: 150 [0/46 (0%)]\tLoss: 3603.779785\n","06:53:11.323491\n","====> Epoch: 150 Average loss: 3803.4226\n","====> Test set loss: 2662.4297\n","Train Epoch: 151 [0/46 (0%)]\tLoss: 3678.320801\n","06:53:21.362918\n","====> Epoch: 151 Average loss: 4971.9185\n","====> Test set loss: 30213.7461\n","Train Epoch: 152 [0/46 (0%)]\tLoss: 20751.513672\n","06:53:31.009150\n","====> Epoch: 152 Average loss: 14362.3389\n","====> Test set loss: 11029.1836\n","Train Epoch: 153 [0/46 (0%)]\tLoss: 10700.187500\n","06:53:40.318463\n","====> Epoch: 153 Average loss: 10918.4346\n","====> Test set loss: 11167.1025\n","Train Epoch: 154 [0/46 (0%)]\tLoss: 11457.015625\n","06:53:50.321035\n","====> Epoch: 154 Average loss: 6602.1597\n","====> Test set loss: 9499.0684\n","Train Epoch: 155 [0/46 (0%)]\tLoss: 5474.257812\n","06:53:59.394941\n","====> Epoch: 155 Average loss: 5649.8613\n","====> Test set loss: 7033.3027\n","Train Epoch: 156 [0/46 (0%)]\tLoss: 6276.588379\n","06:54:09.040701\n","====> Epoch: 156 Average loss: 4839.0620\n","====> Test set loss: 4398.2207\n","Train Epoch: 157 [0/46 (0%)]\tLoss: 4842.622559\n","06:54:18.072591\n","====> Epoch: 157 Average loss: 5707.4419\n","====> Test set loss: 4817.3340\n","Train Epoch: 158 [0/46 (0%)]\tLoss: 5319.607422\n","06:54:28.127765\n","====> Epoch: 158 Average loss: 4059.9756\n","====> Test set loss: 4408.0444\n","Train Epoch: 159 [0/46 (0%)]\tLoss: 2249.812012\n","06:54:37.935893\n","====> Epoch: 159 Average loss: 3924.7690\n","====> Test set loss: 3372.1604\n","Train Epoch: 160 [0/46 (0%)]\tLoss: 4367.760742\n","06:54:47.559958\n","====> Epoch: 160 Average loss: 3717.1125\n","====> Test set loss: 3956.1746\n","Train Epoch: 161 [0/46 (0%)]\tLoss: 3993.984375\n","06:54:56.518432\n","====> Epoch: 161 Average loss: 3287.0669\n","====> Test set loss: 2761.0317\n","Train Epoch: 162 [0/46 (0%)]\tLoss: 3407.697510\n","06:55:06.030127\n","====> Epoch: 162 Average loss: 3126.1587\n","====> Test set loss: 2496.0923\n","Train Epoch: 163 [0/46 (0%)]\tLoss: 3057.178955\n","06:55:15.247419\n","====> Epoch: 163 Average loss: 2643.3765\n","====> Test set loss: 2622.4014\n","Train Epoch: 164 [0/46 (0%)]\tLoss: 1671.714233\n","06:55:24.886630\n","====> Epoch: 164 Average loss: 2077.1863\n","====> Test set loss: 2549.7195\n","Train Epoch: 165 [0/46 (0%)]\tLoss: 2685.569336\n","06:55:33.776110\n","====> Epoch: 165 Average loss: 2555.3191\n","====> Test set loss: 1948.2242\n","Train Epoch: 166 [0/46 (0%)]\tLoss: 1986.015625\n","06:55:43.311181\n","====> Epoch: 166 Average loss: 2506.1047\n","====> Test set loss: 2020.6479\n","Train Epoch: 167 [0/46 (0%)]\tLoss: 2101.327637\n","06:55:52.982377\n","====> Epoch: 167 Average loss: 3115.4258\n","====> Test set loss: 2254.0493\n","Train Epoch: 168 [0/46 (0%)]\tLoss: 2342.466064\n","06:56:01.136706\n","====> Epoch: 168 Average loss: 2532.3838\n","====> Test set loss: 2129.0735\n","Train Epoch: 169 [0/46 (0%)]\tLoss: 3210.937256\n","06:56:11.311787\n","====> Epoch: 169 Average loss: 2865.9912\n","====> Test set loss: 2422.5728\n","Train Epoch: 170 [0/46 (0%)]\tLoss: 2255.984375\n","06:56:20.370731\n","====> Epoch: 170 Average loss: 2926.8569\n","====> Test set loss: 2600.6475\n","Train Epoch: 171 [0/46 (0%)]\tLoss: 3305.733887\n","06:56:30.859244\n","====> Epoch: 171 Average loss: 3123.4746\n","====> Test set loss: 3071.2825\n","Train Epoch: 172 [0/46 (0%)]\tLoss: 2401.372070\n","06:56:40.645581\n","====> Epoch: 172 Average loss: 3421.5110\n","====> Test set loss: 4410.0562\n","Train Epoch: 173 [0/46 (0%)]\tLoss: 8184.458496\n","06:56:50.322219\n","====> Epoch: 173 Average loss: 4451.3735\n","====> Test set loss: 2431.2930\n","Train Epoch: 174 [0/46 (0%)]\tLoss: 2307.175781\n","06:56:59.418494\n","====> Epoch: 174 Average loss: 2556.5896\n","====> Test set loss: 2450.0254\n","Train Epoch: 175 [0/46 (0%)]\tLoss: 3614.091553\n","06:57:09.084423\n","====> Epoch: 175 Average loss: 2620.9512\n","====> Test set loss: 2087.0398\n","Train Epoch: 176 [0/46 (0%)]\tLoss: 1664.916504\n","06:57:18.058337\n","====> Epoch: 176 Average loss: 3631.3601\n","====> Test set loss: 2679.3650\n","Train Epoch: 177 [0/46 (0%)]\tLoss: 4548.916992\n","06:57:28.408970\n","====> Epoch: 177 Average loss: 4447.2944\n","====> Test set loss: 3779.7605\n","Train Epoch: 178 [0/46 (0%)]\tLoss: 4016.958008\n","06:57:38.331764\n","====> Epoch: 178 Average loss: 3975.6284\n","====> Test set loss: 3972.7979\n","Train Epoch: 179 [0/46 (0%)]\tLoss: 5330.759766\n","06:57:47.770923\n","====> Epoch: 179 Average loss: 3523.6648\n","====> Test set loss: 3423.1653\n","Train Epoch: 180 [0/46 (0%)]\tLoss: 4608.749512\n","06:57:57.731909\n","====> Epoch: 180 Average loss: 3235.8733\n","====> Test set loss: 2346.4529\n","Train Epoch: 181 [0/46 (0%)]\tLoss: 4354.765137\n","06:58:08.271988\n","====> Epoch: 181 Average loss: 4234.4453\n","====> Test set loss: 2847.6204\n","Train Epoch: 182 [0/46 (0%)]\tLoss: 5584.919922\n","06:58:17.993356\n","====> Epoch: 182 Average loss: 4540.7129\n","====> Test set loss: 2562.6501\n","Train Epoch: 183 [0/46 (0%)]\tLoss: 2847.895020\n","06:58:27.156818\n","====> Epoch: 183 Average loss: 3100.6455\n","====> Test set loss: 2603.4387\n","Train Epoch: 184 [0/46 (0%)]\tLoss: 3942.410645\n","06:58:36.034354\n","====> Epoch: 184 Average loss: 3845.3325\n","====> Test set loss: 2647.7334\n","Train Epoch: 185 [0/46 (0%)]\tLoss: 6395.313477\n","06:58:46.133362\n","====> Epoch: 185 Average loss: 3587.3906\n","====> Test set loss: 2127.0461\n","Train Epoch: 186 [0/46 (0%)]\tLoss: 3961.167480\n","06:58:56.504192\n","====> Epoch: 186 Average loss: 3239.3530\n","====> Test set loss: 2102.6807\n","Train Epoch: 187 [0/46 (0%)]\tLoss: 2596.276367\n","06:59:05.223458\n","====> Epoch: 187 Average loss: 3136.4338\n","====> Test set loss: 2356.1848\n","Train Epoch: 188 [0/46 (0%)]\tLoss: 3494.626221\n","06:59:14.640452\n","====> Epoch: 188 Average loss: 4654.8135\n","====> Test set loss: 3798.8599\n","Train Epoch: 189 [0/46 (0%)]\tLoss: 8431.645508\n","06:59:25.041515\n","====> Epoch: 189 Average loss: 4691.9116\n","====> Test set loss: 3701.6179\n","Train Epoch: 190 [0/46 (0%)]\tLoss: 3891.191895\n","06:59:34.557076\n","====> Epoch: 190 Average loss: 3284.7048\n","====> Test set loss: 2568.3667\n","Train Epoch: 191 [0/46 (0%)]\tLoss: 2997.270996\n","06:59:44.073315\n","====> Epoch: 191 Average loss: 2898.6880\n","====> Test set loss: 2423.4521\n","Train Epoch: 192 [0/46 (0%)]\tLoss: 3092.340576\n","06:59:53.664791\n","====> Epoch: 192 Average loss: 3694.0391\n","====> Test set loss: 2078.5867\n","Train Epoch: 193 [0/46 (0%)]\tLoss: 2926.565430\n","07:00:02.889885\n","====> Epoch: 193 Average loss: 2980.3608\n","====> Test set loss: 1913.8021\n","Train Epoch: 194 [0/46 (0%)]\tLoss: 3916.592529\n","07:00:12.283433\n","====> Epoch: 194 Average loss: 3503.6257\n","====> Test set loss: 2363.0103\n","Train Epoch: 195 [0/46 (0%)]\tLoss: 3362.804443\n","07:00:21.070132\n","====> Epoch: 195 Average loss: 3214.7666\n","====> Test set loss: 3417.0120\n","Train Epoch: 196 [0/46 (0%)]\tLoss: 4078.585693\n","07:00:30.122155\n","====> Epoch: 196 Average loss: 3349.5591\n","====> Test set loss: 4119.0396\n","Train Epoch: 197 [0/46 (0%)]\tLoss: 6699.920898\n","07:00:39.842971\n","====> Epoch: 197 Average loss: 4127.0410\n","====> Test set loss: 6362.9971\n","Train Epoch: 198 [0/46 (0%)]\tLoss: 13106.439453\n","07:00:49.936076\n","====> Epoch: 198 Average loss: 5155.1079\n","====> Test set loss: 7567.7212\n","Train Epoch: 199 [0/46 (0%)]\tLoss: 5896.865234\n","07:00:59.056659\n","====> Epoch: 199 Average loss: 3898.0615\n","====> Test set loss: 3428.2627\n","Train Epoch: 200 [0/46 (0%)]\tLoss: 4047.495605\n","07:01:08.440593\n","====> Epoch: 200 Average loss: 4473.2329\n","saving the image 6\n","====> Test set loss: 2265.7922\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IdqFO814nFmd"},"source":["Reference : https://github.com/L1aoXingyu/pytorch-beginner/tree/master/08-AutoEncoder"]},{"cell_type":"code","metadata":{"id":"lV2s0SMv2yO3"},"source":["#   model.save_state_dict()\n","torch.save(model.state_dict(), '/content/gdrive/MyDrive/cs259 project/model.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g0Jjq93jYN3C"},"source":["# from google.colab import drive\n","# drive.mount('/content/gdrive') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BfAnSwModHG_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623394881669,"user_tz":420,"elapsed":4757,"user":{"displayName":"William Chong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEeniy7KHOq_LWU1clciKEJDeHZlf8TN47vQoL3g=s64","userId":"04480825896771140250"}},"outputId":"69f26d31-c8a6-4fe1-f0ac-f5a4aa853718"},"source":["model.load_state_dict(torch.load('/content/gdrive/MyDrive/cs259 project/model.pth'))\n","\n","def save_images():\n","  epoch = EPOCHS\n","  model.eval()\n","  test_loss = 0\n","  \n","  for i, (data, _) in enumerate(test_loader):\n","        data = Variable(data, volatile=True)\n","        final, residual_img, upscaled_image, com_img, orig_im = model(data.cuda())\n","        test_loss += loss_function(final, residual_img, upscaled_image, com_img, orig_im).data\n","        if True :\n","#             save_image(final.data[0],'reconstruction_final',nrow=8)\n","#             save_image(com_img.data[0],'com_img',nrow=8)\n","            n = min(data.size(0), 6)\n","            print(\"saving the image \"+str(i))\n","            comparison = torch.cat([data[:n],\n","              final[:n].cpu()])\n","            comparison = comparison.cpu()\n","#             print(comparison.data)\n","            save_image(com_img[i].data,\n","                         '/content/gdrive/MyDrive/cs259 project/results/compressed_' + str(i) +'.png', nrow=n)\n","            save_image(final[i].data,\n","                        '/content/gdrive/MyDrive/cs259 project/results/final_' + str(i) +'.png', nrow=n)\n","            save_image(orig_im[i].data,\n","                        '/content/gdrive/MyDrive/cs259 project/results/original_' + str(i) +'.png', nrow=n)\n","\n","\n","  test_loss /= len(test_loader.dataset)\n","  print('====> Test set loss: {:.4f}'.format(test_loss))\n","\n","print(len(test_loader))\n","save_images()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  if __name__ == '__main__':\n","/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["saving the image 0\n","====> Test set loss: 2265.7925\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Q4jniD80b0gN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623394882224,"user_tz":420,"elapsed":567,"user":{"displayName":"William Chong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEeniy7KHOq_LWU1clciKEJDeHZlf8TN47vQoL3g=s64","userId":"04480825896771140250"}},"outputId":"9d2e9d83-9cc5-4572-ac46-aef0e909078c"},"source":["!git clone https://github.com/ARM-software/SCALE-Sim"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'SCALE-Sim'...\n","remote: Enumerating objects: 407, done.\u001b[K\n","remote: Total 407 (delta 0), reused 0 (delta 0), pack-reused 407\u001b[K\n","Receiving objects: 100% (407/407), 1.23 MiB | 33.17 MiB/s, done.\n","Resolving deltas: 100% (195/195), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cPSvAhZYmzEY","executionInfo":{"status":"ok","timestamp":1623394882493,"user_tz":420,"elapsed":271,"user":{"displayName":"William Chong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEeniy7KHOq_LWU1clciKEJDeHZlf8TN47vQoL3g=s64","userId":"04480825896771140250"}},"outputId":"98372a61-d29e-4b7c-926d-44e2b27048a7"},"source":["!python3 scale.py"],"execution_count":null,"outputs":[{"output_type":"stream","text":["python3: can't open file 'scale.py': [Errno 2] No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zwiVkAi9tpFG","executionInfo":{"status":"ok","timestamp":1623394882650,"user_tz":420,"elapsed":159,"user":{"displayName":"William Chong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEeniy7KHOq_LWU1clciKEJDeHZlf8TN47vQoL3g=s64","userId":"04480825896771140250"}},"outputId":"40f62c86-e976-4b7c-b76b-a37545fc2b57"},"source":["!mv original* '/content/gdrive/MyDrive/cs259 project/'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mv: cannot stat 'original*': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Itle_7s37dWg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623395789385,"user_tz":420,"elapsed":144,"user":{"displayName":"William Chong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjEeniy7KHOq_LWU1clciKEJDeHZlf8TN47vQoL3g=s64","userId":"04480825896771140250"}},"outputId":"dc09576c-c84f-437a-fdb1-ffa39f5e8e53"},"source":["print(\"HELLO\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["HELLO\n"],"name":"stdout"}]}]}